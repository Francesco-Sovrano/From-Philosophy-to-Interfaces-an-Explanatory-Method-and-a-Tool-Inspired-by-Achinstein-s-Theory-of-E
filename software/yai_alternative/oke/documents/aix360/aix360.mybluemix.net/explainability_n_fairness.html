<!DOCTYPE html>
<html lang="en">
    <!-- AI Explainability and Fairness -->
    <div class="tab-pane fade" id="aixf">
        <div class="container-fluid">
            <div class="row">
                <div class="col-md-12">
                        <h2 class="title">AI Explainability and Fairness</h2>
                        <br/>
                        <p>
                            By adding transparency throughout AI systems, explanations can help people examine, identify, and ultimately correct biases and discrimination in machine learning models. When the model is unbiased, effective explanations can assure people of the model fairness and foster trust. 
                        </p>
                        <p>
                            Research shows that people need a diverse set of explanation capabilities to fully scrutinize model biases, which can be supported by algorithms provided in this toolkit. For example, one may want to inspect if there is discrimination in the overall logic of the model. <a href="https://github.com/IBM/AIX360/tree/master/aix360/algorithms/rbm/BRCG.py" target="_blank">Boolean Rule Column Generation</a> and <a href="https://github.com/IBM/AIX360/tree/master/aix360/algorithms/rbm/GLRM.py" target="_blank">Generalized Linear Rule Model</a> can support such global understanding. Others may want to ensure that they are not being unfairly treated by comparing the model's decisions for them to other individuals. <a href="https://github.com/IBM/AIX360/tree/master/aix360/algorithms/contrastive/CEM.py" target="_blank">CEM</a> and <a href="https://github.com/IBM/AIX360/tree/master/aix360/algorithms/protodash/PDASH.py" target="_blank">ProtoDash</a> can help one perform such inspection.
                        </p>
                        <p>
                            To learn more about the effectiveness and user preferences of different explainers for supporting fairness judgments of machine learning models, read a recent paper:
                        </p>
                            <strong>Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K.E. Bellamy, and Casey Dugan</strong>
                            <br/>
                            <a href="https://doi.org/10.1145/3301275.3302310" target="_blank">"Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment"</a>
                            <br/>
                            ACM International Conference on Intelligent User Interfaces, 2019
                        </p>
                        <p>
                            To learn more about AI Fairness and techniques to address biases in AI systems, visit <a href="https://aif360.mybluemix.net" target="_blank">IBM Research AI Fairness 360</a>, an open source toolkit to help you examine, report, and mitigate biases in machine learning models.
                        </p>
                </div>
            </div>
        </div>
    </div>
</html>