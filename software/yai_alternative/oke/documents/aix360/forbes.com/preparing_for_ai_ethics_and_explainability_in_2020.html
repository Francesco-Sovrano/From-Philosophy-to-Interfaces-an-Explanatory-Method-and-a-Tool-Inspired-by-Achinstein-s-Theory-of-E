<html>
	<p>
	People distrust artificial intelligence and in some ways this makes sense. With the desire to create the best performing AI models, many organizations have prioritized complexity over the concepts of explainability and trust. As the world becomes more dependent on algorithms for making a wide range of decisions, technologies and business leaders will be tasked with explaining how a model selected its outcome.
	</p>
	<p>
	Transparency is an essential requirement for generating trust and AI adoption. Meanwhile, regulatory compliance and model security also require companies to design a certain level of interpretability into models. Additionally, a company needs to evaluate the data it uses to ensure that the systems aren't learning and reinforcing unconscious biases. Organizations may need to augment existing data to create a representative sample and account for changes in laws, societal norms and language.
	</p>
	<p>
	Complex AI algorithms allow organizations to unlock insights from data that were previously unattainable. However, the blackbox nature of these systems means it isn't straightforward for business users to understand the logic behind the decision. Even the data scientists that created the model may have trouble explaining why their algorithm made a particular decision. One way to achieve better model transparency is to adopt from a specific family of models that are considered explainable. Examples of these families include linear models, decision trees, rules sets, decision sets, generalized additive models and case-based reasoning methods.
	</p>
	<p>
	Simpler models can often provide the right balance between explainability and performance (e.g., accuracy, enhanced insights). Yet, many companies dislike this approach because they believe a more complex model yields superior results. As a result of these challenges, researchers and technology vendors are working on ways to help companies explain models.
	</p>
	<p>
	There is a diverse range of academics, researchers and technology vendors working on principles and tools to create trusted AI. Preet Gandhi from Nvidia described two classes of explainability techniques in her KDNuggets blog post - ante-hoc and post-hoc. "Ante-hoc techniques entail baking explainability into a model from the beginning. Post-hoc techniques allow models to be trained normally, with explainability only being incorporated at testing time." Over the past six months, I've interviewed a mixture of these individuals to learn more about progress in the AI ethics arena.
	</p>
	<p>
	Algorithmic fairness is a hot topic in AI circles. She also noted that "when it comes to explaining decisions made by algorithms, there is no single approach that works best. There are many ways to explain. The appropriate choice depends on the persona of the consumer and the requirements of the AI pipeline."
	</p>
	<p>
	For example, there is a big difference between the types of explanations desired by people who produce machine learning models such as data scientists and researchers versus the business decision-makers and end-users who consume the information those models produce. A model producer may want explanations that help them improve the models. A business leader wants to understand issues such as data sources and how reliable that output is. A person who received a decision based on a model wants to know what were the contributing factors that led to the answer.
	</p>
	<p>
	To this end, MojsiloviÄ‡ and the IBM team launched AI Explainability 360 in August of 2019. AI Explainability 360 an extensible open-source toolkit that uses various techniques to explain and interpret AI model decision-making. AI Explainability 360 includes 8 state-of-the-art explainability algorithms from IBM Research, along with additional algorithms from the larger research community. There are also industry-specific tutorials. The AI Explainability 360 toolkit offers different explainability algorithms for multiple audiences. For example, in financial services, a loan officer may want to better understand how the AI system came to the recommendation to approve or deny a loan. A bank customer, on the other hand, may require a different explanation about why they were denied or approved for a loan.
	</p>
	<p>
	Tools such as AI Explainability 360 help companies understand what factors went into the decision. For example, if the model declines a new credit card application, the bank should be able to tell the consumer that factors such as a recently missed payment, short credit history, and low credit score were the critical inputs behind in the decision.
	</p>
	<p>
	Another hot topic in AI is eliminating bias which can materialize in many ways and from many different sources. IBM's AI Fairness 360 toolkit, launched in 2018, is an open-source software toolkit that can help detect and remove bias in machine learning models. The toolkit provides developers with algorithms to use to check for unwanted biases. For example, in financial services, a bank would want to know how its model's predictions will affect different groups of people (such as ethnicity, gender, or disability status).
	</p>
	<p>
	Tools such as Fairness 360 would help the bank remove parameters from the model that might cause a bias, such as zip code and ethnicity. Companies building AI models should design with explainability and fairness at the outset and test models throughout the model's lifecycle to ensure fair, trusted outcomes are produced.
	</p>
	<p>
	Business leaders and consumers need to be able to trust the outcomes of advanced artificial intelligence models. Ideally, a company wants to embed explainability into the initial design of a model. However, it's equally important to explain the outcomes of existing models to ensure fairness, check for consistency and enable enhancements in the retraining cycles.
	</p>
	<p>
	As we move to adopt more advanced AI models with deep learning, it's imperative that we also have a set of tools that can help us highlight issues, not just at the time we create the model but also as the model evolves. Just as data governance has become a practice, we'll see organizations create a role and adopt tools for AI governance, if for no other reason than to mitigate potential regulatory risk. While it will take time to create more explainable AI, it's promising to see so many technologists across domains working to solve the problem.
	</p>
</html>